---
layout: post
title: DL相关思考、理解
date: 2020-02-14
Author: Katherinaxxx
tags: [DL]
excerpt: "随时更新对深度学习的思考理解与新发现 "
image: "/images/post/dl/ .jpg"
comments: true
toc: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

* any list
{:toc}

## DL的理论基础

**通用近似定理（Universal approximation theorem）**，其表示拥有无限神经元的单层前馈网络能逼近紧致实数子集上的任意连续函数。通俗来说，只要神经元足够多，单层前馈神经网络「有潜力」逼近任意复杂的连续函数。

DL理论研究可以分为三大类：
* 表征问题（Representation）：为什么深层网络比浅层网络的表达能力更好？
eg.拟合能力、计算复杂度
* 最优化问题（Optimization）：为什么梯度下降能找到很好的极小值解，好的极小值有什么特点？
eg.收敛性分析、landscape分析、极值点分析
* 泛化问题（Generalization）：为什么过参数化仍然能拥有比较好的泛化性，不过拟合？
eg.泛化误差上界、 一致性收敛


## 泛化性

### 传统泛化理论

通常我们所说的泛化误差是指在测试集上的误差，定义如下：

![f](https://katherinaxxx.github.io/images/post/dl/fanhua.jpg#width-full){:height="90%" width="90%"}

它描述的是，泛化误差应该是模型在所有未知数据上的「平均」预测误差，即所有误差的数学期望。注意，我们是无法获取「所有未知数据」的，因此这只是一个定义。

相关理论及新的发现：

* 泛化误差的概率上界来进行，也就是我们常听到的泛化误差上界。
泛化误差上界是样本容量的函数，当样本数增加时，泛化误差上界趋向于 0；同时，泛化误差上界也是模型能力的函数，模型能力越强，它就越难学习，泛化误差上界就越大。

* VC 维，它通过考虑模型函数族的一致性收敛边界，来探讨函数族的表达能力有多强。在 NeurIPS 2019 新方向杰出论文中，研究者表示这种考虑一致性收敛边界的方法，是行不通的。


### 现代泛化理论

通过范数约束函数族，也许我们可以将一致性收敛应用到更简洁与准确的边界：

![f](https://katherinaxxx.github.io/images/post/dl/fanhua2.jpg#width-full){:height="90%" width="90%"}

但本质上仍是一致性收敛的不同表示。在 CMU 的这篇论文中，他们发现，一致性收敛边界实际上并不能全面解释深度学习的泛化问题，我们应该在**一致性收敛**之上来讨论泛化边界。

### 一致性收敛

一致性收敛回答了“为什么降低训练损失能够降低测试损失”这一问题。

**之前的研究大多数都基于一致性收敛考虑泛化边界，但我们的研究表明这类问题很可能是局限的**：

一致性收敛边界会随着参数量的增长而增大，因此这样的边界对于深度网络来说太大了，而且行为也不像真实泛化边界。
即使我们观察到随着数据集的增加，测试误差是正常减少的，但泛化边界却反常地扩大。


## 参数

### 过参数化现象

深度学习存在很强的过参数化现象，其参数量远远超过了数据量。




## 损失函数

L_p 范数距离、总变分距离、Wasserstein 距离、Kolmogorov-Smirnov 距离、Besov IPM
