---
layout: post
title: BigBird
date: 2020-09-29
Author: Katherinaxxx
tags: [NLP]
excerpt: "了解这些网络架构的衍生或许可以加深理解"
image: "/images/post/bigbird/head.png"
comments: true
toc: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

* any list
{:toc}


![bg left:30%](https://katherinaxxx.github.io/images/post/bigbird/bigbird.png)

# Intro & Overview

## Limitations of Previous Transformers-Based Models
$O(n^2)$ quadratic dependency (mainly in terms of memory)
## To remedy this
* BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear $O(n)$
* theoretical analysis
* question answering \summarization\ genomics


---
# Architecture Overview
![](https://katherinaxxx.github.io/images/post/bigbird/architecture.png)

---
# Random Attention
Each query attends over r random number of keys
![w:400px h:200px](https://katherinaxxx.github.io/images/post/bigbird/random.jpeg)
* $O(n^2) \to O(r \cdot n)=O(n)$
* Random graphs are expanders and can approximate complete graphs in a number of different contexts including in their spectral properties
* the shortest path between any two nodes is logarithmic in the number of nodes

---
# Window Attention
A sliding window attention, so that during self attention of width w, query at location i attends from i − w/2 to i + w/2 keys.
![w:320px h:150px](https://katherinaxxx.github.io/images/post/bigbird/window.jpeg)
* $O(n^2) \to O(w \cdot n)=O(n)$
* A great deal of information about a token can be derived from its neighboring tokens.
* Simple Erdos-Renyi random graphs do not have a high clustering coefficient, but small world graphs exhibit high clustering coefficient

---
# Global Attention
* BigBird-ITC
In internal transformer construction (itc), make some existing tokens “global”, which attend over the entire sequence
* BigBird-ETC
In extended transformer construction (etc), include additional “global” tokens such as CLS


# Speed up
![w:800px h:200px](https://katherinaxxx.github.io/images/post/bigbird/speed1.jpg)

---
# Theoretical Result
* Universal Approximators of sequence to sequence functions
![](https://katherinaxxx.github.io/images/post/bigbird/th1.png)
![](https://katherinaxxx.github.io/images/post/bigbird/th2.png)
* Turning Complete
* Limitations
    * Sparse attention mechanisms can not universally replace dense attention mechanisms
    * ![](https://katherinaxxx.github.io/images/post/bigbird/limit.jpg)

---
# Experimental Results
QA
![w:1000px h:600px](https://katherinaxxx.github.io/images/post/bigbird/qa.jpg)

---
Document Classification
![width:90% height:90%](https://katherinaxxx.github.io/images/post/bigbird/docclf.png)

---
Summarization
![](https://katherinaxxx.github.io/images/post/bigbird/summarization.png)

---
Genomics
Longer input sequence handling capability of BigBird would be beneficial as many functional effects in DNA are highly non-local
- Promoter Region Prediction
![w:300px h:200px](https://katherinaxxx.github.io/images/post/bigbird/prp.png)
- Chromatin-Profile Prediction
![w:400px h:250px](https://katherinaxxx.github.io/images/post/bigbird/cpp.png)

---

# Conclusion
* $O(n^2) \to O(n)$
* BigBird satisfies all the known theoretical properties of full transformer
* the extended context modelled by BigBird greatly benefits variety of NLP tasks. (question answering and document summarization)
* introduce a novel application of attention based models where long contexts are beneficial: extracting contextual representations of genomics sequences like DNA.
