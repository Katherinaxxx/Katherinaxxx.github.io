---
layout: post
title: Facebook:Convolutional Sequence to Sequence Learning --- fairseq
date: 2020-03-23
Author: Katherinaxxx
tags: [nlp]
excerpt: "技术、论文、代码、应用"
image: "/images/post/fairseq/haed.jpg"
comments: true
toc: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

* any list
{:toc}

>[fairseq项目地址](https://github.com/pytorch/fairseq)
[fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://www.aclweb.org/anthology/N19-4009/)
[fairseq文档](https://fairseq.readthedocs.io/en/latest/)
[demo](https://www.youtube.com/watch?v=OtgDdWtHvto)
[Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122.pdf)
[参考](https://blog.csdn.net/u012931582/article/details/83719158)

## Convolutional Sequence to Sequence Learning

seq2seq用的是RNN，然而存在**不能并行**的问题

fairseq则是采用CNN，用堆叠CNN来获取上下文的信息从而模拟RNN的效果。这样做的优点则是：
1. 可以并行计算
2. 可以精准控制上下文的长度
3. 卷积和非线性计算次数也是可以固定的

### training

训练模型（英文->德文）如下所示：

![](https://katherinaxxx.github.io/images/post/fairseq/convseq2seq/1.png#width-full){:height="90%" width="90%"}

将英语输入给encoder以计算出keys和values，将德语输入给decoder计算出qury，再结合之前的状态和英语输入来生成下一个词（德语）。训练过程是可以并行的。

下面具体看一下其中用到的结构

#### position embedding

简单来说，输入除了词向量w之外，还加入了位置信息p，最后的输入向量为词向量加上位置向量,即$e=(w_1+p_1,w_2+p_2,...,w_m+p_m)$
在encoder和decoder输出$g=(g_1,...,g_n)$均用了这一方法。
这样做的好处是能够只到正在处理的位置

#### Convolutional Block Structure

1. 用conv叠加的方式就能覆盖上下文，这就不用细说了。

2. 总体看，参数矩阵维数是2dx..，这样最终经过conv block后的输出可以看成两个d维，即A、B两部分
激活函数选用GLU，即 $v([AB])=A\bigotimes \sigma(B)$

3. 为了保证网络够深，还在每个卷积加入residual connection

4. 加入padding保证encoder输入输出尺寸一致；用零向量在输入左侧和右侧padding k − 1个元素，然后从卷积输出的末尾删除k个元素。从而保证没有进一步信息给decoder

5. 线性映射

6. 最后，通过权重为$W_o$且偏置为$b_o$的线性层变换顶部encoder输出$h_{i}^{L}$，计算出T个可能的下一个目标元素$y_{i + 1}$的分布（条件概率）：
$p(y_{i + 1}|y_1,...,y_i,x)=softmax(W_oh_{i}^{L}+b_o) \in \Re_{}^{T} $

#### Multi-step Attention

这种attention的计算如下：

![](https://katherinaxxx.github.io/images/post/fairseq/convseq2seq/3.png#width-full){:height="90%" width="90%"}

![](https://katherinaxxx.github.io/images/post/fairseq/convseq2seq/4.png#width-full){:height="90%" width="90%"}

![](https://katherinaxxx.github.io/images/post/fairseq/convseq2seq/5.png#width-full){:height="90%" width="90%"}

#### Normalization Strategy、 Initialization

通过权重初始化和对网络的各个部分标准化，来稳定学习，以确保整个网络中的方差不会发生巨大的变化。

具体操作看论文3.4和3.5节

### generate/testing

![](https://katherinaxxx.github.io/images/post/fairseq/convseq2seq/2.gif#width-full){:height="90%" width="90%"}

如上图所示，必须一个个生成，因此testing阶段应是不能并行的
此外用beam search择优得到结果

## fairseq

fairseq这个项目应该是从conv seq2seq出发，又结合了后续许多新的研究而成的，具体的恐怕一时半会整理不出来，先看主要的类BaseFairseqModel

### BaseFairseqModel


### 命令
