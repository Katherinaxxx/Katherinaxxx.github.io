---
layout: post
title: nlp、cv论文记录
date: 2020-09-03
Author: Katherinaxxx
tags: [nlp]
excerpt: "简单记录论文思路"
image: "/images/post/paper/LambdaLayer.jpeg"
comments: true
toc: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

* any list
{:toc}

## 简单记录看的最新的论文

### bert系列/transformer

#### [BigBird]()
通过三种attention方式：random attention、window attention、global attention
将二次依赖降到线性

#### [LambdaNetworks](https://openreview.net/pdf?id=xTJEN-ggl1b)
提出Lambda层，将二次依赖降到线性
> In a nutshell, my understanding is the Lambda Layer works using a similar rearranging trick as in "Transformers are RNNs". **Instead of doing attention over positions (i.e. NxN), it ends up doing attention over features (i.e. DxK). That's why it isn't O(N^2).**

![](https://katherinaxxx.github.io/images/post/paper/LambdaLayer.jpeg#width-full){:height="90%" width="90%"}

[LambdaLayer library](https://github.com/lucidrains/lambda-networks):python包 可直接调用。
*ps.代码中用到了torch.einsum和einops.rearrange，有点优秀*

#### [optimal subarchitecture extraction for bert](https://arxiv.org/pdf/2010.10499.pdf)
**总结：** 提取bert的最优子结构。给出了一套“实验方案”，提出了一个$W-coefficient$，通过计算不同结构的$W-coefficient$选出了一个通用最优的子结构（同时考虑了速度、参数量、错误率）。

#### [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)
**总结：** 分解attention matrix。给出了softmax kernel（SM）

![](https://katherinaxxx.github.io/images/post/paper/performer.jpg#width-full){:height="90%" width="90%"}

![](https://katherinaxxx.github.io/images/post/paper/performer.jpeg#width-full){:height="90%" width="90%"}

#### [Is Graph Structure Necessary for Multi-hop Question Answering?](https://www.aclweb.org/anthology/2020.emnlp-main.583.pdf)
**总结：** 通过实验分析graph attention对于阅读理解是否必要。获得的主要结论如下：
1. graph attention是self-attention的特例
2. graph attention的邻接矩阵是先验知识，self-attention可以学到，并且可以学到更多
3. 实体密度大时，graph attention与self-attention差别不大
4. pretrain lm + feature-based时 graph attention效果显著；pretrain lm + fine-tune时 graph attention效果不显著

#### [Language Models are Open Knowledge Graphs]()
**总结：** 从pretrained lm构造知识图谱

### [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity]()

#### [ADDRESSING SOME LIMITATIONS OF TRANSFORMERS WITH FEEDBACK MEMORY](https://arxiv.org/abs/2002.09402)
![](https://katherinaxxx.github.io/images/post/paper/FeedbackTransformer1.jpg#width-full){:height="90%" width="90%"}
![](https://katherinaxxx.github.io/images/post/paper/FeedbackTransformer2.jpg#width-full){:height="90%" width="90%"}
![](https://katherinaxxx.github.io/images/post/paper/FeedbackTransformer3.JPG#width-full){:height="90%" width="90%"}


---
### NER
#### EMNLP2020
* augument
[Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition]()
* pretrain lm
[Coarse-to-Fine Pre-training for Named Entity Recognition]() (MRC-NER)
[Entity Enhanced BERT Pre-training for Chinese NER]()
* model
[Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning]()
### 文本应用---纠错
#### [GECToR – Grammatical Error Correction: Tag, Not Rewrite]()
grmmarly-gector
**总结：** GEC序列标注模型，用transformer encoder。pretrain用人造数据，finetune先用有错语料（平行语料），再用有错和无错的语料（平行语料）。
**创新点：** g-transformations、分阶段finetune

### 语音应用---voice conversion
#### [VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net architecture](https://arxiv.org/abs/2006.04154)

---
### 优化
* [Predictive Coding Approximates Backprop along Arbitrary Computation Graphs]()
