---
layout: post
title: nlp、cv论文记录
date: 2020-09-03
Author: Katherinaxxx
tags: [nlp]
excerpt: "简单记录论文思路"
image: "/images/post/paper/LambdaLayer.jpeg"
comments: true
toc: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

* any list
{:toc}

## 简单记录看的最新的论文

### transformer二次依赖

#### [BigBird]()
通过三种attention方式：random attention、window attention、global attention
将二次依赖降到线性

#### [LambdaNetworks](https://openreview.net/pdf?id=xTJEN-ggl1b)
提出Lambda层，将二次依赖降到线性
> In a nutshell, my understanding is the Lambda Layer works using a similar rearranging trick as in "Transformers are RNNs". **Instead of doing attention over positions (i.e. NxN), it ends up doing attention over features (i.e. DxK). That's why it isn't O(N^2).**

![](https://katherinaxxx.github.io/images/post/paper/LambdaLayer.jpeg#width-full){:height="90%" width="90%"}

[LambdaLayer library](https://github.com/lucidrains/lambda-networks):python包 可直接调用。
*ps.代码中用到了torch.einsum和einops.rearrange，有点优秀*
