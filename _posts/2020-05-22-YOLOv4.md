---
layout: post
title: YOLOv4
date: 2019-12-12
Author: Katherinaxxx
tags: [object detection]
excerpt: "YOLOv4系列最新"
image: "/images/post/yolo/v4/head.jpg"
comments: true
toc: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

* any list
{:toc}

>[YOLOv4论文](https://arxiv.org/pdf/2004.10934.pdf)今年4月新鲜出炉 我拖了两个月了才看。。
[details](https://pjreddie.com/darknet/yolo/)


## YOLOv4: Optimal Speed and Accuracy of Object Detection

### 简介
> We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and com- bine some of them

开篇摘要就说的很清楚了，这篇文章为了提高CNN的精度尝试了各种tricks以及彼此的结合,最终达到SOTA：43.5% AP (65.7% AP50) for the MS COCO dataset at a real- time speed of ∼65 FPS on Tesla V100

文章的主要目标是设计一个在工业生产中运行快的目标检测器，并且可以并行计算，而非是一个理论上计算量少。
也就是说，任何使用常规GPU进行训练和测试的人都可以实现实时，高质量且令人信服的对象检测结果。

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig1.jpg#width-full){:height="90%" width="90%"}

文章的主要贡献在于：
* 1.我们开发了一种高效而强大的目标检测模型。它使每个人都可以使用1080 Ti或2080 Ti GPU训练超快速和准确的物体检测器。
* 2.我们在检测器训练期间，验证了最新的Bag-of- Freebies 和 Bag-of-Specials的影响。
* 3.我们修改了最先进的方法，使它们更有效，更适合单GPU训练，包括CBN ，PAN，SAM等

### 相关方法

因为很多人评价这篇论文是面试必看。。哈哈因为有很多trick以及她们的效果，然后这部分简单介绍了这些模型、方法，因为有很多我也没看或者没用过，所以还会附加查找的资料来解释。

#### 目标检测模型

检测模型通常有两部分组成，backbone（在ImageNet上预训练）和head（用于预测目标的类别和bbox。
关于backbone：在GPU平台上运行通常用VGG、ResNet、ResNeXt、DenseNet，在CPU平台上运行通常是SqueezeNet、MobileNet、ShuffleNet。
> SqueezeNet、MobileNet、ShuffleNet都是轻量级网络，也就是计算复杂度没那么高

关于head，又分为两类one-stage和two-stage。
two-stage典型代表是RCNN系列即fastrcnn、fasterrcnn、R-FCN、Libra R-CNN。twp-stage的anchor-free，RepPoints
one-stage典型代表YOLO、SSD、RetinaNet
one-stage+anchor-free典型代表centernet、cornernet、FCOS

> ps. anchor-free 算法归纳
A.基于多关键点联合表达的方法 -> a.CornerNet/CornerNet-lite：左上角点+右下角点b.ExtremeNet：上下左右4个极值点+中心点 c.CenterNet:Keypoint Triplets for Object Detection：左上角点+右下角点+中心点 d.RepPoints：9个学习到的自适应跳动的采样点 e.FoveaBox：中心点+左上角点+右下角点 f.PLN：4个角点+中心点
B.基于单中心点预测的方法 -> a.CenterNet:Objects as Points：中心点+宽度+高度 b.CSP：中心点+高度（作者预设了目标宽高比固定，根据高度计算出宽度）c.FCOS：中心点+到框的2个距离
anchor-free的缺点：1.正负样本极端不平衡；2.语义模糊性（两个目标中心点重叠）；现在这两者大多是采用Focus Loss和FPN来缓解的

然而，近些年目标检测的研究中又会在backbone和head之间加入neck，通常用于收集不同的feature map，通常是由许多自下而上和自上而下的方式组成。其主要代表是FPN、PAN、BiFPN、NAS-FPN

哇总结了好多，将上面提到的画了一个表如下

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig2.jpg#width-full){:height="90%" width="90%"}

#### bag of freebies （BoF）

将仅改变训练策略或仅增加训练成本的方法，称为“bag of freebies”（“免费赠品”）。常用的有
* 1）数据增强，通过增加输入的多样性从而提高检测器对不同对象和环境的鲁棒性。
* 2）解决数据集的语义分布偏差问题：1>数据分布不平衡，two-stage用hard negative example mining 或 online hard example mining，one-stage用focal loss；2>one-hot很难表达不同类别之间的关联关系,可以用label smoothing
* 3）bbox的目标函数仅把坐标点当作独立的而没有考虑其与目标的整体性，可以用IoU loss、GIoU loss、GIoU loss、CIoU loss

#### Bag of specials （BoS）

对于那些插件模块和后处理方法，这些插件模块和后处理方法仅少量增加推理成本，但可以显着提高对象检测的准确性，我们称它们为“ Bag of specials”（“特价商品”）。
一般而言，这些插件模块用于增强模型中的某些属性，例如扩大感受域，引入注意机制或增强特征集成能力等，而后处理是一种筛选模型预测结果的方法，比如NMS（不过anchor-free的方法就用不到啦）。
常用可以用于扩大感受域的有：SPP、ASPP、RFB
引入注意机制的有：SE、SAM
增强特征集成能力的有：skip connection 或 hyper-column、SFAM, ASFF、BiFPN

### 本文的方法（YOLOv4）

#### 方法论
再次重申重点在于在工程上和并行优化上速度快，而非理论BFLOP上。

#### 选择结构

* 目标1 是在输入网络分辨率，卷积层数，参数数和层输出数量（滤波器数）之间找到最佳平衡。

* 目标2 选择额外blocks来增加感受野，以及将从backbone不同层参数融合的方法 比如FPN、PAN、ASFF、BiFPN

分类任务最优的模型不一定适合目标检测reference，与分类任务相比需要满足以下要求：

* 输入分辨率更高 -- 为了检测多种小物体
* 更多层 -- 分辨率（输入size）更高来那么就要获得更多感受野
* 更多参数 -- 为了保证能检测一张图片中不同size的各种物体

不同size的感受野的作用：
* 和物体同size -- 可以view整个物体
* 和网络同size -- 可以view物体周围的纹理（context）
* 超过网络size -- 增加图片每个点与最终激活的连接

#### 选择BoF和BoS

为了提升目标检测训练效果，CNN网络通常用到一下

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig3.jpg#width-full){:height="90%" width="90%"}

**ps.PReLU和SELU相比更难训练，DropBlock提出时就与其他正则化方法对比了提升明显，syncBN不适用于GPU上训练**

#### 其他提升

为了保证在单个GPU上训练更稳定，做了如下设计和改进：

* 提出一种新的数据增强方法Mosaic, and Self-Adversarial Training（SAT）
* 用遗传算法选择最优超参数
* 修改了一些现有方法，使设计适合进行有效的训练和检测 -- 修改后的SAM，修改后的PAN和交叉小批量标准化（CmBN）

Mosaic指的是混合4个训练图片，SAT也是一种数据增强有两个forwardbackward阶段。

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig5.jpg#width-full){:height="90%" width="90%"}

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig4.jpg#width-full){:height="90%" width="90%"}

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig6.jpg#width-full){:height="90%" width="90%"}

#### YOLOv4

综上，阐述一下YOLOv4的结构

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig7.jpg#width-full){:height="90%" width="90%"}

### 实验和结论

测试了不同训练改进技术（俗称tricks）对ImageNet（ILSVRC 2012 val）数据集上分类器准确性的影响，然后测试了MS COCO（test-dev 2017）数据集上检测器准确性的影响

实验设置具体看论文，实验结果就简单贴一下图

#### 不同features对分类准确性的影响

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig8.jpg#width-full){:height="90%" width="90%"}

#### 不同features对检测器准确性的影响

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig9.jpg#width-full){:height="90%" width="90%"}

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig10.jpg#width-full){:height="90%" width="90%"}

#### 不同backbone和预训练权重对检测器准确性的影响

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig11.jpg#width-full){:height="90%" width="90%"}

#### 不同mini-batch size对检测器准确性的影响

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig12.jpg#width-full){:height="90%" width="90%"}

### 结果、总结

![](https://katherinaxxx.github.io/images/post/yolo/v4/fig13.jpg#width-full){:height="90%" width="90%"}

将YOLOv4对比其他目标检测方法效果如上图所示，四个字总结解释更快更准。SOTA
然后可以在一个GPU上训练，拓展了可用范围。

## 代码

>[github](https://github.com/AlexeyAB/darknet)

搞来搞去发现主流的方法其实也就是这几个，不如仔仔细细看看源码，之前faster rcnn的看过，但是没有记下来，然后。。。就没有然后了 :)



## 读后感

这篇论文虽然没有开创性的创新之处，但是有许多方法、trick总结，以及实验结论，值得学习。YOLOv4就是在YOLO系列下组合或者修改了“新”“优”trick，达到SOTA，比现有目标检测方法更快更准。并且可以在一个GPU上训练使得它可用性提升。
