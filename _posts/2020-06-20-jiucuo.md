---
layout: post
title: nlp领域技术---文本纠错
date: 2020-03-23
Author: Katherinaxxx
tags: [nlp]
excerpt: "技术、论文、代码、应用"
image: "/images/post/jiucuo/head.jpeg"
comments: true
toc: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

* any list
{:toc}

>本文基于一个现有的[中文纠错项目](https://github.com/shibing624/pycorrector)进行学习，首先是项目介绍并补充其中涉及的方法，之后再看代码的实现

## 项目思路介绍与方法

### 规则的解决思路
1. 中文纠错分为两步走，第一步是错误检测，第二步是错误纠正；
2. 错误检测部分先通过结巴中文分词器切词，由于句子中含有错别字，所以切词结果往往会有切分错误的情况，这样从字粒度和词粒度两方面检测错误，
整合这两种粒度的疑似错误结果，形成疑似错误位置候选集；
3. 错误纠正部分，是遍历所有的疑似错误位置，并使用音似、形似词典替换错误位置的词，然后通过语言模型计算句子困惑度，对所有候选集结果比较并排序，得到最优纠正词。

### 深度模型的解决思路
1. 端到端的深度模型可以避免人工提取特征，减少人工工作量，RNN序列模型对文本任务拟合能力强，rnn_attention在英文文本纠错比赛中取得第一名成绩，证明应用效果不错；


2. CRF会计算全局最优输出节点的条件概率，对句子中特定错误类型的检测，会根据整句话判定该错误，阿里参赛2016中文语法纠错任务并取得第一名，证明应用效果不错；
>CRF(Conditional Random Field条件随机场)
与HMM类似，都是计算条件概率，因此都是判别模型。但是HMM假设了两类特征，而CRF是更为抽象的用一个特征函数来表示因此局限更小(无独立性假设)
[introduction to CRF](https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)
[中文introduction](https://zhuanlan.zhihu.com/p/51492037)
[Stanford](https://www.youtube.com/watch?v=rc3YDj5GiVM)从图像分割和nlp角度解读
ps.LR是一个simple CRF

3. seq2seq模型是使用encoder-decoder结构解决序列转换问题，目前在序列转换任务中（如机器翻译、对话生成、文本摘要、图像描述）使用最广泛、效果最好的模型之一。
>通常seq2seq+attention

## Feature
### 模型
* kenlm：kenlm统计语言模型工具
>n-gram语言模型
[KenLM: Faster and Smaller Language Model Queries](https://kheafield.com/papers/avenue/kenlm.pdf)
[项目地址](https://github.com/kpu/kenlm)

* rnn_attention模型：参考Stanford University的nlc模型，该模型是参加2014英文文本纠错比赛并取得第一名的方法
* rnn_crf模型：参考阿里巴巴2016参赛中文语法纠错比赛CGED2018并取得第一名的方法(整理中)
* seq2seq_attention模型：在seq2seq模型加上attention机制，对于长文本效果更好，模型更容易收敛，但容易过拟合
* transformer模型：全attention的结构代替了lstm用于解决sequence to sequence问题，语义特征提取效果更好
>[transformer](https://www.youtube.com/watch?v=ugWDIIOHtPA) seq2seq with self-attention:用self-attention layer 取代RNN（or CNN）

* bert模型：中文fine-tuned模型，使用MASK特征纠正错字
>[BERT and its family - ELMo, BERT, GPT, XLNet, MASS, BART, UniLM, ELECTRA, and more](https://www.youtube.com/watch?v=Bywo7m6ySlk)
[blog](https://katherinaxxx.github.io/blog/nlp/)

* conv_seq2seq模型：基于Facebook出品的fairseq，北京语言大学团队改进ConvS2S模型用于中文纠错，在NLPCC-2018的中文语法纠错比赛中，是唯一使用单模型并取得第三名的成绩
>[fairseq项目地址](https://github.com/pytorch/fairseq)
[fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://www.aclweb.org/anthology/N19-4009/)
[blog](https://katherinaxxx.github.io/blog/fairseq/)

* electra模型：斯坦福和谷歌联合提出的一种更具效率的预训练模型，学习文本上下文表示优于同等计算资源的BERT和XLNet

### 错误检测
* 字粒度：语言模型困惑度（ppl）检测某字的似然概率值低于句子文本平均值，则判定该字是疑似错别字的概率大。
* 词粒度：切词后不在词典中的词是疑似错词的概率大。


### 错误纠正
* 通过错误检测定位所有疑似错误后，取所有疑似错字的音似、形似候选词，
* 使用候选词替换，基于语言模型得到类似翻译模型的候选排序结果，得到最优纠正词。


### 思考
1. 现在的处理手段，在词粒度的错误召回还不错，但错误纠正的准确率还有待提高，更多优质的纠错集及纠错词库会有提升，我更希望算法上有更大的突破。
2. 另外，现在的文本错误不再局限于字词粒度上的拼写错误，需要提高中文语法错误检测（CGED, Chinese Grammar Error Diagnosis）及纠正能力，列在TODO中，后续调研。

## 代码
