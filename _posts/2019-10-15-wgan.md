---
layout: post
title: Wasserstein GAN
date: 2019-10-11
Author: Katherinaxxx
tags: [algorithm]
excerpt: "思想、改进点、理论推导"
image: "/images/pic10.jpg"
comments: true
toc: true
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

* any list
{:toc}

## GAN存在的问题

vanilla GAN最终可以转换成优化JS散度的问题，然而他有许多缺点不利于训练。
Martin Arjovsky， Leon Bottou在[TOWARDS PRINCIPLED METHODS FOR TRAINING
GENERATIVE ADVERSARIAL NETWORKS](https://arxiv.org/pdf/1701.04862.pdf)中从理论角度分析了vanilla GAN存在的问题。

* 训练困难
D如果训练的太好，**梯度消失**G的损失不会降。D训练的不好，G梯度不准。
**梯度消失**
在（近似）最优判别器下，最小化生成器的损失等价于最小化$P_r$与$P_G$之间的JS散度，而由于$P_r$与$P_G$几乎不可能有不可忽略的重叠，所以无论它们相距多远JS散度都是常数$log2$，最终导致生成器的梯度（近似）为0，梯度消失。

> 当$P_r$与$P_G$的支撑集（support）是高维空间中的低维流形（manifold）时，$P_r$与$P_G$重叠部分测度（measure）为0的概率为1

* 最小化第二种生成器loss函数，会等价于最小化一个不合理的距离衡量，导致两个问题，一是梯度不稳定，二是collapse mode即多样性不足


并且，指出了改进点。在此基础上又推了一堆公式定理给出了[Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf)最终算法流程。


## Wasserstein GAN

### 解决问题

* 彻底解决GAN训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度

* 基本解决了collapse mode的问题，确保了生成样本的多样性
* 训练过程中终于有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表GAN训练得越好，代表生成器产生的图像质量越高
* 以上一切好处不需要精心设计的网络架构（DCGAN就是通过对网络架构进行实验枚举找到较好的网络架构），最简单的多层全连接网络就可以做到

### 相比vanilla GAN改进点

* 判别器最后一层去掉sigmoid
WGAN中的判别器做的是近似拟合Wasserstein距离，属于回归任务，所以要把最后一层的sigmoid拿掉(后面详细解释)
* 生成器和判别器的loss不取log
* 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c
* 不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行（实验得到的结论，trick）

综上，给出WGAN算法

![algorithm](https://katherinaxxx.github.io/images/post/wgan/algorithm.jpg#width-full){:height="90%" width="90%"}


### Wasserstein Distance

#### 定义、特性

[Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric)也叫Earth Mover’s Distance。**用Wasserstein Distance代替JS散度，同时完成了稳定训练和进程指标的问题。** 定义如下

![wddef](https://katherinaxxx.github.io/images/post/wgan/wddef.jpg#width-full){:height="90%" width="90%"}

**Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近。** 论文中给出了例子,考虑二维空间中的两个分布$P_1$和$P_2$，$P_1$在线段AB上均匀分布，$P_2$在线段CD上均匀分布，通过控制参数$\theta$可以控制着两个分布的距离远近。

![graph](https://katherinaxxx.github.io/images/post/wgan/graph.jpg#width-full){:height="60%" width="60%"}

此时Wasserstein距离、JS散度、KL散度分别为

![calc](https://katherinaxxx.github.io/images/post/wgan/calc.jpg#width-full){:height="70%" width="70%"}

KL散度和JS散度是突变的，要么最大要么最小，Wasserstein距离却是平滑的，作图更明显

![wdjs](https://katherinaxxx.github.io/images/post/wgan/wdjs.jpg#width-full){:height="90%" width="90%"}

如果我们要用梯度下降法优化参数，前两者根本提供不了梯度，Wasserstein距离却可以。类似地，在高维空间中如果两个分布不重叠或者重叠部分可忽略，则KL和JS既反映不了远近，也提供不了梯度，但是Wasserstein却可以提供有意义的梯度。

#### 计算 Wasserstein GAN
但是关于他的计算就很复杂了，不过[Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf)论文里证明了

![wd](https://katherinaxxx.github.io/images/post/wgan/wd.jpg#width-full){:height="80%" width="80%"}

**<font color="#d344！6">推导待补充！</font><br />**

##### 1-Lipschitz

Lipschitz连续条件限制了一个连续函数的最大局部变动幅度。
可以先直观的想一下，在生成图像时，D的输出意味着与真实图像的接近程度，D会给真实图像较大值、给其他较小值。如果D变化剧烈，将使生成器倾向于生成与真实图像一模一样的图像，导致**多样性不高**。 1-Lipschitz的作用就是限制D的变化要更平缓一些，直观上是可行的。

##### weight clipping

紧接着需要考虑如何保证D满足1-Lipschitz，这里提出weight clipping。其实是如下操作：

对于NN中的所有参数或权重，在更新梯度后，事先选中某个常数c
* 如果权重 w>c，则赋值 w←c
* 如果权重 w<−c，则赋值 w←c

直觉上，如果神经网络的权重都限制在一定的范围[-c,c]内，那么网络的输出也会被限定在一定范围内。换句话说，这个网络会属于某个 K-Lipschitz。当然，我们并不确定K是多少，并且这样的函数也不一定能使 $E_{x\backsim P_{data}}[D(x)]−E_{x\backsim P_G}[D(x)]$最大化。

不管怎么说吧，这就是原版WGAN的方法，对vanilla GAN的具大提升。在新版WGAN中提出用gradient penalty，详细请参考在此不赘述。




## 参考

[令人拍案叫绝的Wasserstein GAN](https://zhuanlan.zhihu.com/p/25071913)
[WGAN 笔记](https://lotabout.me/2018/WGAN/)
[Wasserstein GAN and the Kantorovich-Rubinstein Duality](https://vincentherrmann.github.io/blog/wasserstein/)
